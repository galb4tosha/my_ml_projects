{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow.keras import Model, Input, optimizers\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.applications.resnet_v2 import ResNet50V2\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = \"../../data/raw/training.csv\"\n",
    "IMGS_SIZE = (96, 96)\n",
    "EPOCHS = 200\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before drop na 7049\n",
      "after drop na 7049\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(DATASET_PATH)\n",
    "print(\"before drop na\", len(data.index))\n",
    "data = data.fillna(method=\"ffill\")\n",
    "print(\"after drop na\", len(data.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_array = np.array([data['Image'][i].split(' ') for i in range(len(data))], dtype='float')\n",
    "img_array = np.array([img.reshape(96,96,1) for img in img_array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keypoints = data.drop([\"Image\"], axis=1).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(img_array, keypoints, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get simple convolutional model (val mse 1.28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 96, 96, 1)]       0         \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 94, 94, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 47, 47, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 45, 45, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 22, 22, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 20, 20, 8)         1160      \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 3200)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 30)                96030     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 102,134\n",
      "Trainable params: 102,134\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(96, 96, 1))\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\")(input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(16, (3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(8, (3, 3), activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "output = Dense(30, activation=\"relu\")(x)\n",
    "model = Model(inputs=input, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "vat_acc_checkpoint = ModelCheckpoint(\n",
    "    \"best_simple_model.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "89/89 [==============================] - 1s 7ms/step - loss: 10.8251 - val_loss: 8.7994\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 7.3092 - val_loss: 6.8148\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 6.9036 - val_loss: 6.6932\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 6.7848 - val_loss: 6.4644\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 6.0558 - val_loss: 6.5168\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 5.8010 - val_loss: 6.2517\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 5.6300 - val_loss: 5.5254\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 4.7035 - val_loss: 7.1447\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 4.9117 - val_loss: 4.1583\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 3.5426 - val_loss: 6.4050\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 3.4725 - val_loss: 5.6771\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 2.3082 - val_loss: 1.8894\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.7413 - val_loss: 1.6657\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.5331 - val_loss: 1.5554\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.4492 - val_loss: 1.5076\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.3884 - val_loss: 1.5148\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.3464 - val_loss: 1.4668\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.3167 - val_loss: 1.4532\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.2884 - val_loss: 1.4117\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.2671 - val_loss: 1.4362\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.2524 - val_loss: 1.3945\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.2382 - val_loss: 1.3804\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.2259 - val_loss: 1.3738\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.2035 - val_loss: 1.3713\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1944 - val_loss: 1.4457\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1901 - val_loss: 1.3453\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1710 - val_loss: 1.3410\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.1600 - val_loss: 1.3482\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1565 - val_loss: 1.3319\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1380 - val_loss: 1.3313\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.1369 - val_loss: 1.3503\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.1256 - val_loss: 1.3527\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1202 - val_loss: 1.3470\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.1177 - val_loss: 1.3560\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.1100 - val_loss: 1.3431\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.1061 - val_loss: 1.3599\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0990 - val_loss: 1.3477\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0954 - val_loss: 1.3140\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0769 - val_loss: 1.3102\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0808 - val_loss: 1.3406\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0743 - val_loss: 1.3263\n",
      "Epoch 42/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0711 - val_loss: 1.3316\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0913 - val_loss: 1.3798\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0721 - val_loss: 1.3351\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0626 - val_loss: 1.3409\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0562 - val_loss: 1.4115\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0597 - val_loss: 1.3408\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0448 - val_loss: 1.3363\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0377 - val_loss: 1.3111\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0383 - val_loss: 1.3604\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0368 - val_loss: 1.3018\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0242 - val_loss: 1.3362\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0292 - val_loss: 1.3047\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0276 - val_loss: 1.2977\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0160 - val_loss: 1.3095\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0046 - val_loss: 1.3538\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0096 - val_loss: 1.2821\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0156 - val_loss: 1.3137\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 1.0078 - val_loss: 1.3010\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0020 - val_loss: 1.3021\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0130 - val_loss: 1.3007\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9888 - val_loss: 1.3168\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9937 - val_loss: 1.2898\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 1.0118 - val_loss: 1.3132\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9865 - val_loss: 1.2990\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9834 - val_loss: 1.2871\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9885 - val_loss: 1.2953\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9811 - val_loss: 1.3121\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9766 - val_loss: 1.3477\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9904 - val_loss: 1.3261\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9688 - val_loss: 1.2900\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9638 - val_loss: 1.3068\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9628 - val_loss: 1.3004\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9624 - val_loss: 1.3522\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9644 - val_loss: 1.3252\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9605 - val_loss: 1.3059\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9468 - val_loss: 1.3201\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9549 - val_loss: 1.3343\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9522 - val_loss: 1.3171\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9424 - val_loss: 1.3755\n",
      "Epoch 81/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9503 - val_loss: 1.4040\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9389 - val_loss: 1.3459\n",
      "Epoch 83/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9304 - val_loss: 1.3116\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9367 - val_loss: 1.2993\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9321 - val_loss: 1.3225\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9386 - val_loss: 1.3527\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9345 - val_loss: 1.3016\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9267 - val_loss: 1.3541\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9304 - val_loss: 1.3664\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9220 - val_loss: 1.3258\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9231 - val_loss: 1.3163\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9222 - val_loss: 1.3285\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9202 - val_loss: 1.3184\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9170 - val_loss: 1.2987\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9106 - val_loss: 1.3525\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9069 - val_loss: 1.3121\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9069 - val_loss: 1.3267\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9500 - val_loss: 1.3213\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.9022 - val_loss: 1.3306\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9011 - val_loss: 1.3298\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9003 - val_loss: 1.3107\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8880 - val_loss: 1.3138\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8886 - val_loss: 1.3208\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8914 - val_loss: 1.3387\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8932 - val_loss: 1.3606\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.9033 - val_loss: 1.3240\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8873 - val_loss: 1.3226\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8751 - val_loss: 1.3559\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8847 - val_loss: 1.3271\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8841 - val_loss: 1.3608\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8942 - val_loss: 1.3286\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8734 - val_loss: 1.3376\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8910 - val_loss: 1.3452\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8766 - val_loss: 1.3239\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8713 - val_loss: 1.3082\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8639 - val_loss: 1.3672\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8786 - val_loss: 1.3657\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8713 - val_loss: 1.3876\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8657 - val_loss: 1.3684\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8724 - val_loss: 1.3366\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8664 - val_loss: 1.3474\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8697 - val_loss: 1.3464\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8687 - val_loss: 1.3694\n",
      "Epoch 124/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8597 - val_loss: 1.3308\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8447 - val_loss: 1.3245\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8655 - val_loss: 1.3358\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8594 - val_loss: 1.3611\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8534 - val_loss: 1.3271\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8602 - val_loss: 1.3464\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8763 - val_loss: 1.3259\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8344 - val_loss: 1.3479\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8523 - val_loss: 1.3387\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8509 - val_loss: 1.3642\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8480 - val_loss: 1.3226\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8536 - val_loss: 1.3301\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8355 - val_loss: 1.3342\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8396 - val_loss: 1.3234\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8429 - val_loss: 1.3554\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8479 - val_loss: 1.3749\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8352 - val_loss: 1.3345\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8376 - val_loss: 1.3629\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8425 - val_loss: 1.3699\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8274 - val_loss: 1.3386\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8271 - val_loss: 1.3499\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8214 - val_loss: 1.4004\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8430 - val_loss: 1.3731\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8244 - val_loss: 1.3346\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8201 - val_loss: 1.3638\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8277 - val_loss: 1.3318\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8207 - val_loss: 1.3543\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8268 - val_loss: 1.3732\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8299 - val_loss: 1.3589\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8215 - val_loss: 1.3497\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8203 - val_loss: 1.3527\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8175 - val_loss: 1.3552\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8112 - val_loss: 1.3627\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8195 - val_loss: 1.3671\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8152 - val_loss: 1.3465\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8161 - val_loss: 1.3462\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8194 - val_loss: 1.3624\n",
      "Epoch 161/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8225 - val_loss: 1.3697\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8086 - val_loss: 1.4262\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8171 - val_loss: 1.3516\n",
      "Epoch 164/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8084 - val_loss: 1.3622\n",
      "Epoch 165/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8034 - val_loss: 1.3488\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7975 - val_loss: 1.3625\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8056 - val_loss: 1.3579\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7991 - val_loss: 1.3688\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7974 - val_loss: 1.3793\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8178 - val_loss: 1.4130\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.8138 - val_loss: 1.3717\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8096 - val_loss: 1.3511\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7970 - val_loss: 1.3780\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.8090 - val_loss: 1.3718\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7917 - val_loss: 1.3622\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7900 - val_loss: 1.3521\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7946 - val_loss: 1.4064\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7968 - val_loss: 1.3619\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7920 - val_loss: 1.3611\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7862 - val_loss: 1.3986\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7992 - val_loss: 1.3655\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7887 - val_loss: 1.3782\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7875 - val_loss: 1.4073\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7925 - val_loss: 1.4288\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7958 - val_loss: 1.3674\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7767 - val_loss: 1.3675\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7853 - val_loss: 1.3804\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7831 - val_loss: 1.4044\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7813 - val_loss: 1.3785\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7740 - val_loss: 1.3897\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7891 - val_loss: 1.3772\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7918 - val_loss: 1.3842\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7959 - val_loss: 1.3769\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7875 - val_loss: 1.3783\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7855 - val_loss: 1.3852\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7765 - val_loss: 1.3743\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7665 - val_loss: 1.3647\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7852 - val_loss: 1.3585\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 0s 4ms/step - loss: 0.7706 - val_loss: 1.3814\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 0s 5ms/step - loss: 0.7741 - val_loss: 1.4193\n"
     ]
    }
   ],
   "source": [
    "hostory = model.fit(X_train,\n",
    "                    Y_train,\n",
    "                    validation_data=(X_test, Y_test), \n",
    "                    epochs=EPOCHS, \n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    callbacks=[vat_acc_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 2ms/step - loss: 1.2821\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1.2820613384246826"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"best_simple_model.h5\")\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get vgg like model. Val loss 2.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 96, 96, 1)]       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 94, 94, 32)        320       \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPooling  (None, 47, 47, 32)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 45, 45, 64)        18496     \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 22, 22, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 20, 20, 128)       73856     \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 18, 18, 128)       147584    \n",
      "                                                                 \n",
      " max_pooling2d_5 (MaxPooling  (None, 9, 9, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 10368)             0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 30)                311070    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 551,326\n",
      "Trainable params: 551,326\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "input = Input(shape=(96, 96, 1))\n",
    "x = Conv2D(32, (3, 3), activation=\"relu\")(input)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(64, (3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\")(x)\n",
    "x = Conv2D(128, (3, 3), activation=\"relu\")(x)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "# x = Dense(5000, activation=\"relu\")(x)\n",
    "# x = Dense(1000, activation=\"relu\")(x)\n",
    "output = Dense(30, activation=\"relu\")(x)\n",
    "vgg_like_model = Model(inputs=input, outputs=output)\n",
    "vgg_like_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_like_model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "vat_acc_checkpoint = ModelCheckpoint(\n",
    "    \"best_vgg_like_model.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "89/89 [==============================] - 1s 10ms/step - loss: 14.9382 - val_loss: 12.8831\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 11.2372 - val_loss: 10.4364\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 11.0329 - val_loss: 10.0709\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 10.1411 - val_loss: 9.7969\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 9.4181 - val_loss: 8.7134\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 6.9690 - val_loss: 6.7183\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 5.9443 - val_loss: 4.7926\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.8572 - val_loss: 3.4088\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.3124 - val_loss: 3.2653\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.1934 - val_loss: 3.3940\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 3.0822 - val_loss: 3.1800\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.9932 - val_loss: 3.1459\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.9534 - val_loss: 3.0319\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.9026 - val_loss: 3.1186\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.8702 - val_loss: 3.0460\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.8363 - val_loss: 2.9877\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.8149 - val_loss: 2.9378\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7811 - val_loss: 2.9717\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7731 - val_loss: 2.9586\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7362 - val_loss: 3.2483\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7426 - val_loss: 2.9372\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7115 - val_loss: 2.9869\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.7048 - val_loss: 2.9406\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.9857 - val_loss: 2.9828\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6779 - val_loss: 3.1291\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6980 - val_loss: 2.9299\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6501 - val_loss: 3.0416\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6664 - val_loss: 3.0230\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6501 - val_loss: 3.0956\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6461 - val_loss: 2.9845\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6336 - val_loss: 2.9775\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6182 - val_loss: 2.9711\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6173 - val_loss: 2.9295\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6152 - val_loss: 2.9289\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5944 - val_loss: 3.0203\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5909 - val_loss: 2.9345\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5372 - val_loss: 3.1956\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.6102 - val_loss: 3.0711\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5639 - val_loss: 3.0805\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5985 - val_loss: 2.9576\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5449 - val_loss: 2.9136\n",
      "Epoch 42/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5367 - val_loss: 2.8936\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5060 - val_loss: 3.2229\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5856 - val_loss: 3.0540\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5031 - val_loss: 2.9994\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.5145 - val_loss: 2.9896\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4920 - val_loss: 3.0086\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4809 - val_loss: 2.9124\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4967 - val_loss: 2.9898\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4919 - val_loss: 2.9606\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4609 - val_loss: 2.9382\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4567 - val_loss: 3.3484\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.4926 - val_loss: 3.0444\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.0623 - val_loss: 2.5147\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9791 - val_loss: 2.4420\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9928 - val_loss: 2.4708\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9700 - val_loss: 2.4657\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9522 - val_loss: 2.5346\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9340 - val_loss: 2.5140\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9855 - val_loss: 2.5314\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9169 - val_loss: 2.4999\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9451 - val_loss: 2.4318\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9352 - val_loss: 2.5153\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9491 - val_loss: 2.4534\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9140 - val_loss: 2.4662\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8969 - val_loss: 2.5137\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9575 - val_loss: 2.8275\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.0872 - val_loss: 2.4616\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9738 - val_loss: 2.4690\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.1395 - val_loss: 2.4536\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8768 - val_loss: 2.4405\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.8870 - val_loss: 2.4268\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8842 - val_loss: 2.4417\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 1s 9ms/step - loss: 1.8589 - val_loss: 2.4262\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8616 - val_loss: 2.4545\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8414 - val_loss: 2.4819\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8588 - val_loss: 2.6065\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 2.0318 - val_loss: 2.4311\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8893 - val_loss: 2.4647\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8237 - val_loss: 2.4603\n",
      "Epoch 81/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8430 - val_loss: 2.4430\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9554 - val_loss: 2.4853\n",
      "Epoch 83/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8647 - val_loss: 2.4320\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8554 - val_loss: 2.5851\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8878 - val_loss: 2.4397\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8395 - val_loss: 2.4446\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8681 - val_loss: 2.5831\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8358 - val_loss: 2.6302\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.9070 - val_loss: 2.4845\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8414 - val_loss: 2.4659\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8206 - val_loss: 2.4662\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8301 - val_loss: 2.4289\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8250 - val_loss: 2.4456\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8077 - val_loss: 2.5334\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8145 - val_loss: 2.5187\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8007 - val_loss: 2.4573\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8077 - val_loss: 2.4359\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8019 - val_loss: 2.4539\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8157 - val_loss: 2.4390\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7862 - val_loss: 2.4314\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7917 - val_loss: 2.4545\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8092 - val_loss: 2.5713\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8152 - val_loss: 2.4569\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8158 - val_loss: 2.4536\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7756 - val_loss: 2.5207\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7927 - val_loss: 2.4425\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7774 - val_loss: 2.4719\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7936 - val_loss: 2.4550\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7625 - val_loss: 2.4381\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8191 - val_loss: 2.4460\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.8029 - val_loss: 2.5364\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7860 - val_loss: 2.5234\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7672 - val_loss: 2.4769\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7765 - val_loss: 2.4510\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7538 - val_loss: 2.4548\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7573 - val_loss: 2.4273\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7625 - val_loss: 2.4148\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7397 - val_loss: 2.5916\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7713 - val_loss: 2.4986\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7648 - val_loss: 2.4671\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7712 - val_loss: 2.4315\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7358 - val_loss: 2.4401\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7635 - val_loss: 2.4554\n",
      "Epoch 124/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7694 - val_loss: 2.4429\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7391 - val_loss: 2.5074\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7495 - val_loss: 2.4463\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7438 - val_loss: 2.5142\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7567 - val_loss: 2.4339\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7112 - val_loss: 2.5196\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7577 - val_loss: 2.4560\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7375 - val_loss: 2.4744\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7313 - val_loss: 2.4224\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7189 - val_loss: 2.5622\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7454 - val_loss: 2.4688\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7557 - val_loss: 2.4338\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7128 - val_loss: 2.4778\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7521 - val_loss: 2.4906\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7459 - val_loss: 2.4292\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7089 - val_loss: 2.4260\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7240 - val_loss: 2.4224\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7052 - val_loss: 2.4508\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7247 - val_loss: 2.4314\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7297 - val_loss: 2.4331\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7288 - val_loss: 2.4286\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7157 - val_loss: 2.4619\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7071 - val_loss: 2.4492\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7102 - val_loss: 2.4283\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7138 - val_loss: 2.4629\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7112 - val_loss: 2.4461\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6875 - val_loss: 2.4445\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7410 - val_loss: 2.4446\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7366 - val_loss: 2.4406\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7058 - val_loss: 2.4241\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7038 - val_loss: 2.4319\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7099 - val_loss: 2.4422\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7144 - val_loss: 2.5513\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7062 - val_loss: 2.4385\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7235 - val_loss: 2.5175\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7104 - val_loss: 2.4879\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6763 - val_loss: 2.4409\n",
      "Epoch 161/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7074 - val_loss: 2.4311\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6797 - val_loss: 2.4957\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7077 - val_loss: 2.4597\n",
      "Epoch 164/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6775 - val_loss: 2.4533\n",
      "Epoch 165/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6882 - val_loss: 2.4852\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6955 - val_loss: 2.4174\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6828 - val_loss: 2.4917\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6925 - val_loss: 2.5140\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6839 - val_loss: 2.4291\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6913 - val_loss: 2.4869\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6741 - val_loss: 2.5069\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7086 - val_loss: 2.4299\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6856 - val_loss: 2.4715\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6745 - val_loss: 2.4107\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6806 - val_loss: 2.4234\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6681 - val_loss: 2.4901\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6922 - val_loss: 2.4222\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6814 - val_loss: 2.4701\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6553 - val_loss: 2.4286\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6622 - val_loss: 2.4424\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6824 - val_loss: 2.4239\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6750 - val_loss: 2.4524\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6715 - val_loss: 2.4262\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6715 - val_loss: 2.5486\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7353 - val_loss: 2.4424\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6745 - val_loss: 2.4359\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6507 - val_loss: 2.4194\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6702 - val_loss: 2.4432\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6766 - val_loss: 2.4227\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6353 - val_loss: 2.4157\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6409 - val_loss: 2.4288\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6913 - val_loss: 2.4658\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6726 - val_loss: 2.4189\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6588 - val_loss: 2.4217\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6457 - val_loss: 2.4478\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6598 - val_loss: 2.4616\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.7875 - val_loss: 2.5445\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6893 - val_loss: 2.4157\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6300 - val_loss: 2.4464\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 1s 8ms/step - loss: 1.6501 - val_loss: 2.4233\n"
     ]
    }
   ],
   "source": [
    "hostory = vgg_like_model.fit(X_train,\n",
    "                                Y_train,\n",
    "                                validation_data=(X_test, Y_test), \n",
    "                                epochs=EPOCHS, \n",
    "                                batch_size=BATCH_SIZE,\n",
    "                                callbacks=[vat_acc_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 2ms/step - loss: 2.4107\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2.4107491970062256"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"best_vgg_like_model.h5\")\n",
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ResNetV2 base model. Val loss 0.74"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_one_channel_to_three(img):\n",
    "    tree_chanels = cv2.merge((img, img, img))\n",
    "    return tree_chanels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "three_chanels_train_dataset = np.array([convert_one_channel_to_three(img) for img in X_train])\n",
    "three_chanels_test_dataset = np.array([convert_one_channel_to_three(img) for img in X_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 96, 96, 3)]       0         \n",
      "                                                                 \n",
      " resnet50v2 (Functional)     (None, 3, 3, 2048)        23564800  \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 1, 1, 2048)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 30)                61470     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 23,626,270\n",
      "Trainable params: 23,580,830\n",
      "Non-trainable params: 45,440\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model = ResNet50V2(input_shape=(96, 96, 3),\n",
    "                            include_top=False,\n",
    "                            weights='imagenet',\n",
    "                            classifier_activation=None)\n",
    "inputs = Input(shape=(96, 96, 3))\n",
    "x = base_model(inputs)\n",
    "x = MaxPooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "output = Dense(30, activation=\"relu\")(x)\n",
    "res_net_base_model = Model(inputs=inputs, outputs=output)\n",
    "res_net_base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_net_base_model.compile(optimizer=\"adam\", loss=\"mean_absolute_error\")\n",
    "vat_acc_checkpoint = ModelCheckpoint(\n",
    "    \"resnet_base_model.h5\", monitor=\"val_loss\", mode=\"min\", save_best_only=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "89/89 [==============================] - 4s 44ms/step - loss: 0.2793 - val_loss: 1.7435\n",
      "Epoch 2/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.3014 - val_loss: 1.4626\n",
      "Epoch 3/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2913 - val_loss: 1.7316\n",
      "Epoch 4/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2840 - val_loss: 1.4428\n",
      "Epoch 5/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2825 - val_loss: 1.4008\n",
      "Epoch 6/200\n",
      "89/89 [==============================] - 4s 46ms/step - loss: 0.2734 - val_loss: 0.7744\n",
      "Epoch 7/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2859 - val_loss: 1.3668\n",
      "Epoch 8/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2810 - val_loss: 2.4899\n",
      "Epoch 9/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2865 - val_loss: 1.4332\n",
      "Epoch 10/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2725 - val_loss: 1.5609\n",
      "Epoch 11/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2930 - val_loss: 2.6968\n",
      "Epoch 12/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2687 - val_loss: 1.4438\n",
      "Epoch 13/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2721 - val_loss: 0.7785\n",
      "Epoch 14/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2666 - val_loss: 0.7974\n",
      "Epoch 15/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2634 - val_loss: 1.0038\n",
      "Epoch 16/200\n",
      "89/89 [==============================] - 4s 45ms/step - loss: 0.2569 - val_loss: 0.7702\n",
      "Epoch 17/200\n",
      "89/89 [==============================] - 4s 46ms/step - loss: 0.2635 - val_loss: 0.7607\n",
      "Epoch 18/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2671 - val_loss: 1.2831\n",
      "Epoch 19/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2745 - val_loss: 2.9637\n",
      "Epoch 20/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2709 - val_loss: 1.6688\n",
      "Epoch 21/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2669 - val_loss: 0.8813\n",
      "Epoch 22/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2706 - val_loss: 1.0889\n",
      "Epoch 23/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2951 - val_loss: 3.3051\n",
      "Epoch 24/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2703 - val_loss: 0.9382\n",
      "Epoch 25/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2668 - val_loss: 0.8606\n",
      "Epoch 26/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2575 - val_loss: 1.1946\n",
      "Epoch 27/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2736 - val_loss: 1.3196\n",
      "Epoch 28/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2625 - val_loss: 0.7805\n",
      "Epoch 29/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2534 - val_loss: 0.8382\n",
      "Epoch 30/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2427 - val_loss: 1.2450\n",
      "Epoch 31/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2608 - val_loss: 1.0431\n",
      "Epoch 32/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2657 - val_loss: 0.9394\n",
      "Epoch 33/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2766 - val_loss: 2.1241\n",
      "Epoch 34/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2637 - val_loss: 0.8493\n",
      "Epoch 35/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2572 - val_loss: 0.8594\n",
      "Epoch 36/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2568 - val_loss: 1.6137\n",
      "Epoch 37/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2517 - val_loss: 2.8595\n",
      "Epoch 38/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2855 - val_loss: 0.9422\n",
      "Epoch 39/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2515 - val_loss: 1.5534\n",
      "Epoch 40/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2463 - val_loss: 2.5205\n",
      "Epoch 41/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2435 - val_loss: 0.8937\n",
      "Epoch 42/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2485 - val_loss: 1.4897\n",
      "Epoch 43/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2519 - val_loss: 1.0273\n",
      "Epoch 44/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2489 - val_loss: 2.4877\n",
      "Epoch 45/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2538 - val_loss: 1.2209\n",
      "Epoch 46/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2430 - val_loss: 1.6915\n",
      "Epoch 47/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2483 - val_loss: 1.0542\n",
      "Epoch 48/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2380 - val_loss: 2.0627\n",
      "Epoch 49/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2359 - val_loss: 0.7728\n",
      "Epoch 50/200\n",
      "89/89 [==============================] - 4s 45ms/step - loss: 0.2461 - val_loss: 0.7483\n",
      "Epoch 51/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2484 - val_loss: 0.7827\n",
      "Epoch 52/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2488 - val_loss: 2.5864\n",
      "Epoch 53/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2546 - val_loss: 1.7840\n",
      "Epoch 54/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2465 - val_loss: 1.7277\n",
      "Epoch 55/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2465 - val_loss: 0.8719\n",
      "Epoch 56/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2384 - val_loss: 1.0402\n",
      "Epoch 57/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2451 - val_loss: 0.8373\n",
      "Epoch 58/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2405 - val_loss: 1.3416\n",
      "Epoch 59/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2342 - val_loss: 0.8003\n",
      "Epoch 60/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2402 - val_loss: 0.9967\n",
      "Epoch 61/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2288 - val_loss: 0.9203\n",
      "Epoch 62/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2285 - val_loss: 1.9337\n",
      "Epoch 63/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2407 - val_loss: 0.8854\n",
      "Epoch 64/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2433 - val_loss: 1.8263\n",
      "Epoch 65/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2400 - val_loss: 1.9441\n",
      "Epoch 66/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2425 - val_loss: 2.3558\n",
      "Epoch 67/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2320 - val_loss: 0.9127\n",
      "Epoch 68/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2295 - val_loss: 1.4522\n",
      "Epoch 69/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2313 - val_loss: 3.0704\n",
      "Epoch 70/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2267 - val_loss: 1.4536\n",
      "Epoch 71/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2327 - val_loss: 2.2076\n",
      "Epoch 72/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2340 - val_loss: 1.0847\n",
      "Epoch 73/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2225 - val_loss: 1.1849\n",
      "Epoch 74/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2364 - val_loss: 1.1902\n",
      "Epoch 75/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2256 - val_loss: 0.9544\n",
      "Epoch 76/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2332 - val_loss: 0.8339\n",
      "Epoch 77/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2288 - val_loss: 3.2264\n",
      "Epoch 78/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2573 - val_loss: 1.4876\n",
      "Epoch 79/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2324 - val_loss: 0.8595\n",
      "Epoch 80/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2255 - val_loss: 1.8669\n",
      "Epoch 81/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2312 - val_loss: 2.1505\n",
      "Epoch 82/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2488 - val_loss: 2.8339\n",
      "Epoch 83/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2348 - val_loss: 1.1456\n",
      "Epoch 84/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2323 - val_loss: 1.1702\n",
      "Epoch 85/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2258 - val_loss: 2.0803\n",
      "Epoch 86/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2235 - val_loss: 1.1531\n",
      "Epoch 87/200\n",
      "89/89 [==============================] - 4s 45ms/step - loss: 0.2276 - val_loss: 0.7394\n",
      "Epoch 88/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2220 - val_loss: 2.2607\n",
      "Epoch 89/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2234 - val_loss: 1.6362\n",
      "Epoch 90/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2251 - val_loss: 0.8126\n",
      "Epoch 91/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2477 - val_loss: 1.0989\n",
      "Epoch 92/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2329 - val_loss: 1.1507\n",
      "Epoch 93/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2185 - val_loss: 1.2991\n",
      "Epoch 94/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2196 - val_loss: 1.3322\n",
      "Epoch 95/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2186 - val_loss: 0.8272\n",
      "Epoch 96/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2329 - val_loss: 0.7743\n",
      "Epoch 97/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2264 - val_loss: 0.9290\n",
      "Epoch 98/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2179 - val_loss: 0.8559\n",
      "Epoch 99/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2147 - val_loss: 1.1973\n",
      "Epoch 100/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2172 - val_loss: 0.7772\n",
      "Epoch 101/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2152 - val_loss: 0.7438\n",
      "Epoch 102/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2175 - val_loss: 1.5290\n",
      "Epoch 103/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2204 - val_loss: 1.5132\n",
      "Epoch 104/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2202 - val_loss: 0.9035\n",
      "Epoch 105/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2143 - val_loss: 1.7943\n",
      "Epoch 106/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2101 - val_loss: 1.9438\n",
      "Epoch 107/200\n",
      "89/89 [==============================] - 4s 39ms/step - loss: 0.2042 - val_loss: 1.3512\n",
      "Epoch 108/200\n",
      "89/89 [==============================] - 4s 39ms/step - loss: 0.2085 - val_loss: 2.7438\n",
      "Epoch 109/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2128 - val_loss: 1.1474\n",
      "Epoch 110/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2134 - val_loss: 0.9210\n",
      "Epoch 111/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2109 - val_loss: 2.1590\n",
      "Epoch 112/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2057 - val_loss: 2.6024\n",
      "Epoch 113/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2215 - val_loss: 1.2398\n",
      "Epoch 114/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2144 - val_loss: 0.9491\n",
      "Epoch 115/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2096 - val_loss: 2.3751\n",
      "Epoch 116/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2115 - val_loss: 1.6067\n",
      "Epoch 117/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2072 - val_loss: 0.7799\n",
      "Epoch 118/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2168 - val_loss: 1.4502\n",
      "Epoch 119/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2071 - val_loss: 2.6111\n",
      "Epoch 120/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2088 - val_loss: 0.9365\n",
      "Epoch 121/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2079 - val_loss: 1.0990\n",
      "Epoch 122/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2099 - val_loss: 1.3510\n",
      "Epoch 123/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2057 - val_loss: 1.1134\n",
      "Epoch 124/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2155 - val_loss: 1.0354\n",
      "Epoch 125/200\n",
      "89/89 [==============================] - 4s 39ms/step - loss: 0.2166 - val_loss: 1.2455\n",
      "Epoch 126/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2032 - val_loss: 0.9861\n",
      "Epoch 127/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2006 - val_loss: 3.0498\n",
      "Epoch 128/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1991 - val_loss: 1.1096\n",
      "Epoch 129/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1954 - val_loss: 1.4665\n",
      "Epoch 130/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1986 - val_loss: 2.0236\n",
      "Epoch 131/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2018 - val_loss: 0.9838\n",
      "Epoch 132/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2024 - val_loss: 1.4780\n",
      "Epoch 133/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2105 - val_loss: 2.9256\n",
      "Epoch 134/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2144 - val_loss: 0.8309\n",
      "Epoch 135/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1995 - val_loss: 1.6618\n",
      "Epoch 136/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2176 - val_loss: 3.5183\n",
      "Epoch 137/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2110 - val_loss: 0.9891\n",
      "Epoch 138/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1999 - val_loss: 0.7564\n",
      "Epoch 139/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2014 - val_loss: 1.1939\n",
      "Epoch 140/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2125 - val_loss: 0.7471\n",
      "Epoch 141/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2082 - val_loss: 1.8722\n",
      "Epoch 142/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2060 - val_loss: 1.9109\n",
      "Epoch 143/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1909 - val_loss: 0.8708\n",
      "Epoch 144/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2056 - val_loss: 2.8348\n",
      "Epoch 145/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1938 - val_loss: 1.1633\n",
      "Epoch 146/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.1969 - val_loss: 1.4076\n",
      "Epoch 147/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2063 - val_loss: 1.9920\n",
      "Epoch 148/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.1967 - val_loss: 0.7944\n",
      "Epoch 149/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2005 - val_loss: 0.9039\n",
      "Epoch 150/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2027 - val_loss: 1.5134\n",
      "Epoch 151/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2008 - val_loss: 1.4957\n",
      "Epoch 152/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.2056 - val_loss: 1.0006\n",
      "Epoch 153/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2117 - val_loss: 1.3359\n",
      "Epoch 154/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2012 - val_loss: 1.2305\n",
      "Epoch 155/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1962 - val_loss: 0.8227\n",
      "Epoch 156/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1847 - val_loss: 2.1740\n",
      "Epoch 157/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2002 - val_loss: 1.2512\n",
      "Epoch 158/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1952 - val_loss: 0.8368\n",
      "Epoch 159/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1932 - val_loss: 1.4733\n",
      "Epoch 160/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2012 - val_loss: 1.0102\n",
      "Epoch 161/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1978 - val_loss: 1.0669\n",
      "Epoch 162/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1960 - val_loss: 0.9645\n",
      "Epoch 163/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1877 - val_loss: 1.0010\n",
      "Epoch 164/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1992 - val_loss: 2.4653\n",
      "Epoch 165/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1844 - val_loss: 3.2182\n",
      "Epoch 166/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1915 - val_loss: 0.8074\n",
      "Epoch 167/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1932 - val_loss: 2.2402\n",
      "Epoch 168/200\n",
      "89/89 [==============================] - 4s 40ms/step - loss: 0.1958 - val_loss: 1.5406\n",
      "Epoch 169/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1948 - val_loss: 0.8811\n",
      "Epoch 170/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1851 - val_loss: 0.8235\n",
      "Epoch 171/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1919 - val_loss: 0.7686\n",
      "Epoch 172/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1998 - val_loss: 0.7682\n",
      "Epoch 173/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1915 - val_loss: 1.7065\n",
      "Epoch 174/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1966 - val_loss: 1.3468\n",
      "Epoch 175/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1925 - val_loss: 2.1306\n",
      "Epoch 176/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1998 - val_loss: 1.2186\n",
      "Epoch 177/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2023 - val_loss: 2.7928\n",
      "Epoch 178/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1905 - val_loss: 0.8187\n",
      "Epoch 179/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1914 - val_loss: 0.7813\n",
      "Epoch 180/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1821 - val_loss: 1.4975\n",
      "Epoch 181/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1911 - val_loss: 1.5976\n",
      "Epoch 182/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1873 - val_loss: 1.4558\n",
      "Epoch 183/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1883 - val_loss: 1.3139\n",
      "Epoch 184/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1904 - val_loss: 2.2829\n",
      "Epoch 185/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1881 - val_loss: 1.5836\n",
      "Epoch 186/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1828 - val_loss: 0.7905\n",
      "Epoch 187/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1889 - val_loss: 1.8054\n",
      "Epoch 188/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1838 - val_loss: 0.7897\n",
      "Epoch 189/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1800 - val_loss: 1.0239\n",
      "Epoch 190/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1869 - val_loss: 1.3647\n",
      "Epoch 191/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1834 - val_loss: 1.4313\n",
      "Epoch 192/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1871 - val_loss: 1.4684\n",
      "Epoch 193/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1945 - val_loss: 0.9311\n",
      "Epoch 194/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1982 - val_loss: 1.6385\n",
      "Epoch 195/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1851 - val_loss: 3.0483\n",
      "Epoch 196/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.2014 - val_loss: 2.4259\n",
      "Epoch 197/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1896 - val_loss: 1.6672\n",
      "Epoch 198/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1862 - val_loss: 1.0957\n",
      "Epoch 199/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1837 - val_loss: 0.8991\n",
      "Epoch 200/200\n",
      "89/89 [==============================] - 3s 39ms/step - loss: 0.1864 - val_loss: 1.7048\n"
     ]
    }
   ],
   "source": [
    "hostory = res_net_base_model.fit(three_chanels_train_dataset,\n",
    "                                    Y_train,\n",
    "                                    validation_data=(three_chanels_test_dataset, Y_test), \n",
    "                                    epochs=EPOCHS, \n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    callbacks=[vat_acc_checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 9ms/step - loss: 0.7394\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.739396333694458"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = load_model(\"resnet_base_model.h5\")\n",
    "model.evaluate(three_chanels_test_dataset, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 1s 8ms/step\n"
     ]
    }
   ],
   "source": [
    "pred = model.predict(three_chanels_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([63, 37, 29, 40, 59, 36, 73, 34, 37, 39, 22, 40, 53, 29, 81, 27, 38,\n",
       "       33, 15, 36, 48, 65, 72, 71, 34, 77, 51, 76, 48, 76])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_int = np.array(pred[0], dtype=int)\n",
    "pred_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f1bb42bc070>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "all_points = list() \n",
    "for i in range(0, 30, 2):\n",
    "    points = (pred_int[i], pred_int[i+1])\n",
    "    all_points.append(points)\n",
    "\n",
    "image = X_test[0]\n",
    "for point in all_points:\n",
    "    image = cv2.circle(image, point, 1, (255, 0, 0), 2)\n",
    "plt.imshow(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6dbe6877d5ec30e6af1e1055811c2ac5b743fb92f98193534da0acfda60dc502"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
